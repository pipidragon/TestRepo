=====================
Modelling Methodology
=====================

=========================================
(A) Data Preprocessing (Domain Knowledge)
=========================================
1. Data Cleaning
-Remove Duplicates
-Handle Missing Values
-Detect & Remove Outlier
2. Data Exploration and Visualization
-Understand Data Distribution
-Identify Relationships between variables
3. Feature Engineering
-Create New Features based on domain knowledge
-Transform Features (e.g. log, square root, etc., if needed)
4. Normalization And Standardization
-Normalize or standardize to ensure they are on a similar scale.
5. Address Multicollinearity
-Identify highly correlated features
-Combine or remove highly correlated features to reduce multicollinearity

=========================
(B) Modeling & Evaluation
=========================
6. Train Model
-Model Selection
-Train Model
7. Evaluate Model
-Use appropriate metrics (e.g., accuracy, precision, recall, RMSE) to evaluate model performance.
-Cross-Validation: Perform cross-validation to ensure the model generalizes well.
8. Hyperparameter Tuning
-Optimize Parameters: Use techniques like Grid Search or Random Search to find the best hyperparameters.
9. Model Validation
-Test on Holdout Set: Validate the model on a separate test set to check for overfitting.
10. Repeat and Improve
-Iterate: If the model is underperforming, revisit previous steps to improve it.
11. Deployment
-Model Deployment: Deploy the model to a production environment.
-Monitor Performance: Continuously monitor the modelâ€™s performance and retrain if necessary.

==========
Prediction
==========
12. Prediction
-Apply exactly same transformation on the new data before fit the model with the data to get the forecast
13. New Data Integration
-Integrate new data into the model to ensure the model performance


==========
Detailed.
==========
3. Feature Engineering:
A) For Categorial Variables
	[a] One-Hot Encoding:	For Lowly cardinal data
	[b] Label Encoding:	For Lowly cardinal data (imp: model might interpret it as ordinal relationship)
	[c] Frequency Encoding:	For non-distinct dataset (e.g. inappropriate if appear only once per date)
	[d] Target Encoding:	Replace the categorial variables with the grouped mean of the target variables (e.g. inappropriate if appear only once per date)
	[e] Aggregation:	Group the data across the data (e.g. weekly/monthly/quarterly/yearly mean, median, standard deviation)
	[f] Embedding:		Only if the naming is meaningful
	[g] Hashing:		For Highly cardinal data (e.g. convert string to hash code and modulo by bins(n), 3 ^ n)
A) For massive amount distinct categorial variables (can use any of below)
	[a] Use Hierarchical Model
	[b] Feature Engineering:
		@ Aggregates Statistics: Group the data across the date and (timely mean, median, standard deviation) [Weekly, Monthly, Quarterly, Yearly, remove after feature importance]
		@ Temporal Features: Further explained below
	[c] Use NN or pre-trained NN
	[d] Hybrid
	[e] Possible challenges
		@ Computational intensive
		@ Time Consuming
		@ Complex and non explainable (use SHAP/LIME to overcome)
		@ Overfitting
B) For Temporal Features:
	a) Lagged Variables
	b) Rolling statistics: rolling mean and standard deviation
	c) Date Components (day, month, year, week, days of week, weekday weekend) [This captures trend]
	d) Seasonality: is_summer [0, 1], or is_cny [0, 1]

4. Normalization & Standardization (X only, Y no need)
[A] Min-Max Scaling: Best for NN
[B] Z- Score Standardization: Best when data is normally distributed
[C] Max Absolute Scaling: Best for sparse data (many 0 or missing values)
[D] Robust Scaling: Best for data containing outlier
[E:EX1] df.skew(), +ive=right-skewed, -ive=left-skewed, closer to 0 = less skewed
[E1] Log or Exp Transformation: For heavily right/left-skewed data
[E2] ^(1/n) or ^n: For mild right/left-skewed data
[E3] Box-Cox Transformation: Can handle both right and left skewed data
[E4] Yeo-Johnson Transformation: Can handle 0 and negative values
[F] Decimal Scaling: Best when precision and interpretability is important

4B. Transformation (Reduce complexity)
[A] Clustering: Unsupervised
[B] Classification: Supervised
-First use scatter plot to determine the data pattern
-A & B convert continuous values to class/cluster, then further apply encoding technique




5. Address Multicollinearity 
-Multicollinearity leads to higher beta on highly correlated variables, model sensitive to those variables
-Reduce multicollinearity to increase model's stability & performance
A) Check correlation
B) For highly correlated variables (e.g. X1 & X2)
	a) remain only 1
	b) combine into 1 (X1+X2 or X1*X2)
C) Train the model with all (X1, X2, X1+X2 & X1*X2)
D) check the feature importance
E) Based on correlation and feature importance, remove less important feature
F) Optional to further apply Ridge or Lasso Regression to reduce multicollinearity

==========
Prediction
==========
12. Apply exact transformation with the training model, can save as pipeline
13. Integrate new data into the model to ensure performance
[a] Retrain the whole mode: Computational Intensive
[b] Incremental Learning: Direct update model with new data. Not applicable to all model
[c] No update the model: Forecast might be outdated
